{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prroy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "\n",
    "n_inputs = height * width * channels\n",
    "\n",
    "conv1_filters = 96\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_padding = 'SAME'\n",
    "\n",
    "conv2_filters = 96\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_padding = 'SAME'\n",
    "\n",
    "conv3_filters = 192\n",
    "conv3_ksize = 3\n",
    "conv3_stride = 1\n",
    "conv3_padding = 'SAME'\n",
    "\n",
    "conv4_filters = 192\n",
    "conv4_ksize = 3\n",
    "conv4_stride = 2\n",
    "conv4_padding = 'SAME'\n",
    "\n",
    "conv5_filters = 192\n",
    "conv5_ksize = 1\n",
    "conv5_stride = 1\n",
    "conv5_padding = 'VALID'\n",
    "\n",
    "conv6_filters = 10\n",
    "conv6_ksize = 1\n",
    "conv6_stride = 1\n",
    "conv6_padding = 'VALID'\n",
    "\n",
    "n_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(name='X', dtype=tf.float32, shape=(None, height, width, channels))\n",
    "y = tf.placeholder(name='y', dtype=tf.int32, shape=(None))\n",
    "\n",
    "conv1 = tf.layers.conv2d(X, filters=conv1_filters, kernel_size=conv1_ksize, strides=conv1_stride, padding=conv1_padding,\n",
    "                        name='conv1', activation=tf.nn.relu)\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv1_filters, kernel_size=conv1_ksize, strides=conv1_stride, padding=conv1_padding,\n",
    "                        name='conv2', activation=tf.nn.relu)\n",
    "conv3 = tf.layers.conv2d(conv2, filters=conv2_filters, kernel_size=conv2_ksize, strides=conv2_stride, padding=conv2_padding,\n",
    "                        name='conv3', activation=tf.nn.relu)\n",
    "drop1 = tf.layers.dropout(conv3, rate=0.5)\n",
    "conv4 = tf.layers.conv2d(drop1, filters=conv3_filters, kernel_size=conv3_ksize, strides=conv3_stride, padding=conv3_padding,\n",
    "                       name='conv4', activation=tf.nn.relu)\n",
    "conv5 = tf.layers.conv2d(conv4, filters=conv3_filters, kernel_size=conv3_ksize, strides=conv3_stride, padding=conv3_padding,\n",
    "                        name='conv5', activation=tf.nn.relu)\n",
    "conv6 = tf.layers.conv2d(conv5, filters=conv4_filters, kernel_size=conv4_ksize, strides=conv4_stride, padding=conv4_padding,\n",
    "                       name='conv6', activation=tf.nn.relu)\n",
    "drop2 = tf.layers.dropout(conv6, rate=0.5)\n",
    "conv7 = tf.layers.conv2d(drop2, filters=conv3_filters, kernel_size=conv3_ksize, strides=conv3_stride, padding=conv3_padding,\n",
    "                        name='conv7', activation=tf.nn.relu)\n",
    "conv8 = tf.layers.conv2d(conv7, filters=conv5_filters, kernel_size=conv5_ksize, strides=conv5_stride, padding=conv5_padding,\n",
    "                        name='conv8', activation=tf.nn.relu)\n",
    "conv9 = tf.layers.conv2d(conv8, filters=conv6_filters, kernel_size=conv6_ksize, strides=conv6_stride, padding=conv6_padding,\n",
    "                        name='conv9')\n",
    "pool1 = tf.layers.average_pooling2d(conv9, pool_size=8, strides=8, padding='VALID')\n",
    "\n",
    "logits = tf.reshape(pool1, shape=(-1, 10))\n",
    "y_prob = tf.nn.softmax(logits, name='y_prob')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "#momentum = 0.9\n",
    "\n",
    "\n",
    "with tf.name_scope('Train'):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "batches.append(unpickle('../../../../prroy/datasets/CIFAR10/cifar-10-batches-py/data_batch_1'))\n",
    "batches.append(unpickle('../../../../prroy/datasets/CIFAR10/cifar-10-batches-py/data_batch_2'))\n",
    "batches.append(unpickle('../../../../prroy/datasets/CIFAR10/cifar-10-batches-py/data_batch_3'))\n",
    "batches.append(unpickle('../../../../prroy/datasets/CIFAR10/cifar-10-batches-py/data_batch_4'))\n",
    "batches.append(unpickle('../../../../prroy/datasets/CIFAR10/cifar-10-batches-py/data_batch_5'))\n",
    "batches.append(unpickle('../../../../prroy/datasets/CIFAR10/cifar-10-batches-py/test_batch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(batch):\n",
    "    return batches[batch][b'data'].reshape((-1,3,32,32)).transpose((0,2,3,1)), np.array(batches[batch][b'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_data, test_batch_label = fetch_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch1, y_batch1 = fetch_batch(1)\n",
    "X_batch2, y_batch2 = fetch_batch(2)\n",
    "X_batch3, y_batch3 = fetch_batch(3)\n",
    "X_batch4, y_batch4 = fetch_batch(4)\n",
    "X_batch5, y_batch5 = fetch_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = np.concatenate((X_batch1, X_batch2, X_batch3, X_batch4, X_batch5))\n",
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels = np.concatenate((y_batch1, y_batch2, y_batch3, y_batch4, y_batch5))\n",
    "y_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3) (40000,) (10000, 32, 32, 3) (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_labels, test_size=0.2, random_state=0)\n",
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(starting_index, batch_size):\n",
    "    return X_train[starting_index: min(starting_index + batch_size, len(X_train))],\\\n",
    "        y_train[starting_index: min(starting_index + batch_size, len(y_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training accuracy: 0.421875, validation accuracy: 0.45399999618530273\n",
      "epoch: 1, training accuracy: 0.484375, validation accuracy: 0.5425000190734863\n",
      "epoch: 2, training accuracy: 0.546875, validation accuracy: 0.5945000052452087\n",
      "epoch: 3, training accuracy: 0.625, validation accuracy: 0.6274999976158142\n",
      "epoch: 4, training accuracy: 0.75, validation accuracy: 0.6541000008583069\n",
      "epoch: 5, training accuracy: 0.765625, validation accuracy: 0.6661999821662903\n",
      "epoch: 6, training accuracy: 0.765625, validation accuracy: 0.6820999979972839\n",
      "epoch: 7, training accuracy: 0.796875, validation accuracy: 0.6955000162124634\n",
      "epoch: 8, training accuracy: 0.828125, validation accuracy: 0.704200029373169\n",
      "epoch: 9, training accuracy: 0.84375, validation accuracy: 0.7103000283241272\n",
      "epoch: 10, training accuracy: 0.859375, validation accuracy: 0.7156000137329102\n",
      "epoch: 11, training accuracy: 0.890625, validation accuracy: 0.7164000272750854\n",
      "epoch: 12, training accuracy: 0.890625, validation accuracy: 0.7196999788284302\n",
      "epoch: 13, training accuracy: 0.9375, validation accuracy: 0.7251999974250793\n",
      "epoch: 14, training accuracy: 0.96875, validation accuracy: 0.7276999950408936\n",
      "epoch: 15, training accuracy: 0.953125, validation accuracy: 0.7236999869346619\n",
      "epoch: 16, training accuracy: 0.953125, validation accuracy: 0.7117000222206116\n",
      "epoch: 17, training accuracy: 0.9375, validation accuracy: 0.7069000005722046\n",
      "epoch: 18, training accuracy: 0.96875, validation accuracy: 0.7476999759674072\n",
      "epoch: 19, training accuracy: 0.96875, validation accuracy: 0.7366999983787537\n",
      "epoch: 20, training accuracy: 0.984375, validation accuracy: 0.7480999827384949\n",
      "epoch: 21, training accuracy: 1.0, validation accuracy: 0.7477999925613403\n",
      "epoch: 22, training accuracy: 1.0, validation accuracy: 0.7276999950408936\n",
      "epoch: 23, training accuracy: 0.984375, validation accuracy: 0.703499972820282\n",
      "epoch: 24, training accuracy: 0.9375, validation accuracy: 0.6879000067710876\n",
      "epoch: 25, training accuracy: 0.984375, validation accuracy: 0.7110999822616577\n",
      "epoch: 26, training accuracy: 1.0, validation accuracy: 0.7069000005722046\n",
      "epoch: 27, training accuracy: 0.96875, validation accuracy: 0.7200999855995178\n",
      "epoch: 28, training accuracy: 0.984375, validation accuracy: 0.7630000114440918\n",
      "epoch: 29, training accuracy: 1.0, validation accuracy: 0.7405999898910522\n",
      "epoch: 30, training accuracy: 0.984375, validation accuracy: 0.7305999994277954\n",
      "epoch: 31, training accuracy: 1.0, validation accuracy: 0.7411999702453613\n",
      "epoch: 32, training accuracy: 1.0, validation accuracy: 0.7486000061035156\n",
      "epoch: 33, training accuracy: 1.0, validation accuracy: 0.7609000205993652\n",
      "epoch: 34, training accuracy: 1.0, validation accuracy: 0.7688999772071838\n",
      "epoch: 35, training accuracy: 1.0, validation accuracy: 0.766700029373169\n",
      "epoch: 36, training accuracy: 1.0, validation accuracy: 0.7631000280380249\n",
      "epoch: 37, training accuracy: 0.96875, validation accuracy: 0.7228000164031982\n",
      "epoch: 38, training accuracy: 1.0, validation accuracy: 0.7159000039100647\n",
      "epoch: 39, training accuracy: 1.0, validation accuracy: 0.7146000266075134\n",
      "epoch: 40, training accuracy: 0.984375, validation accuracy: 0.6474000215530396\n",
      "epoch: 41, training accuracy: 1.0, validation accuracy: 0.6934000253677368\n",
      "epoch: 42, training accuracy: 1.0, validation accuracy: 0.7448999881744385\n",
      "epoch: 43, training accuracy: 1.0, validation accuracy: 0.7457000017166138\n",
      "epoch: 44, training accuracy: 1.0, validation accuracy: 0.7269999980926514\n",
      "epoch: 45, training accuracy: 1.0, validation accuracy: 0.7462000250816345\n",
      "epoch: 46, training accuracy: 1.0, validation accuracy: 0.7240999937057495\n",
      "epoch: 47, training accuracy: 1.0, validation accuracy: 0.7645999789237976\n",
      "epoch: 48, training accuracy: 1.0, validation accuracy: 0.7551000118255615\n",
      "epoch: 49, training accuracy: 1.0, validation accuracy: 0.7620000243186951\n",
      "epoch: 50, training accuracy: 1.0, validation accuracy: 0.7591000199317932\n",
      "epoch: 51, training accuracy: 1.0, validation accuracy: 0.7559000253677368\n",
      "epoch: 52, training accuracy: 1.0, validation accuracy: 0.7512999773025513\n",
      "epoch: 53, training accuracy: 1.0, validation accuracy: 0.7613999843597412\n",
      "epoch: 54, training accuracy: 1.0, validation accuracy: 0.7652000188827515\n",
      "epoch: 55, training accuracy: 1.0, validation accuracy: 0.7626000046730042\n",
      "epoch: 56, training accuracy: 1.0, validation accuracy: 0.7717000246047974\n",
      "epoch: 57, training accuracy: 1.0, validation accuracy: 0.760699987411499\n",
      "epoch: 58, training accuracy: 1.0, validation accuracy: 0.7552000284194946\n",
      "epoch: 59, training accuracy: 1.0, validation accuracy: 0.7551000118255615\n",
      "epoch: 60, training accuracy: 1.0, validation accuracy: 0.7734000086784363\n",
      "epoch: 61, training accuracy: 1.0, validation accuracy: 0.7534000277519226\n",
      "epoch: 62, training accuracy: 1.0, validation accuracy: 0.7601000070571899\n",
      "epoch: 63, training accuracy: 1.0, validation accuracy: 0.7684000134468079\n",
      "epoch: 64, training accuracy: 1.0, validation accuracy: 0.767799973487854\n",
      "epoch: 65, training accuracy: 1.0, validation accuracy: 0.7631000280380249\n",
      "epoch: 66, training accuracy: 1.0, validation accuracy: 0.7609999775886536\n",
      "epoch: 67, training accuracy: 1.0, validation accuracy: 0.7746000289916992\n",
      "epoch: 68, training accuracy: 1.0, validation accuracy: 0.7615000009536743\n",
      "epoch: 69, training accuracy: 1.0, validation accuracy: 0.7692000269889832\n",
      "epoch: 70, training accuracy: 1.0, validation accuracy: 0.7644000053405762\n",
      "epoch: 71, training accuracy: 1.0, validation accuracy: 0.7795000076293945\n",
      "epoch: 72, training accuracy: 1.0, validation accuracy: 0.7545999884605408\n",
      "epoch: 73, training accuracy: 1.0, validation accuracy: 0.7791000008583069\n",
      "epoch: 74, training accuracy: 1.0, validation accuracy: 0.786300003528595\n",
      "epoch: 75, training accuracy: 1.0, validation accuracy: 0.7809000015258789\n",
      "epoch: 76, training accuracy: 1.0, validation accuracy: 0.7520999908447266\n",
      "epoch: 77, training accuracy: 1.0, validation accuracy: 0.7713000178337097\n",
      "epoch: 78, training accuracy: 1.0, validation accuracy: 0.7613999843597412\n",
      "epoch: 79, training accuracy: 1.0, validation accuracy: 0.7781999707221985\n",
      "epoch: 80, training accuracy: 1.0, validation accuracy: 0.7710999846458435\n",
      "epoch: 81, training accuracy: 1.0, validation accuracy: 0.7427999973297119\n",
      "epoch: 82, training accuracy: 1.0, validation accuracy: 0.772599995136261\n",
      "epoch: 83, training accuracy: 1.0, validation accuracy: 0.7594000101089478\n",
      "epoch: 84, training accuracy: 1.0, validation accuracy: 0.7594000101089478\n",
      "epoch: 85, training accuracy: 1.0, validation accuracy: 0.7641000151634216\n",
      "epoch: 86, training accuracy: 1.0, validation accuracy: 0.7720999717712402\n",
      "epoch: 87, training accuracy: 1.0, validation accuracy: 0.7756999731063843\n",
      "epoch: 88, training accuracy: 1.0, validation accuracy: 0.7702000141143799\n",
      "epoch: 89, training accuracy: 1.0, validation accuracy: 0.7702000141143799\n",
      "epoch: 90, training accuracy: 1.0, validation accuracy: 0.7767999768257141\n",
      "epoch: 91, training accuracy: 1.0, validation accuracy: 0.766700029373169\n",
      "epoch: 92, training accuracy: 1.0, validation accuracy: 0.7702999711036682\n",
      "epoch: 93, training accuracy: 1.0, validation accuracy: 0.7746000289916992\n",
      "epoch: 94, training accuracy: 1.0, validation accuracy: 0.7782999873161316\n",
      "epoch: 95, training accuracy: 1.0, validation accuracy: 0.7753000259399414\n",
      "epoch: 96, training accuracy: 1.0, validation accuracy: 0.769599974155426\n",
      "epoch: 97, training accuracy: 1.0, validation accuracy: 0.766700029373169\n",
      "epoch: 98, training accuracy: 1.0, validation accuracy: 0.758899986743927\n",
      "epoch: 99, training accuracy: 1.0, validation accuracy: 0.7813000082969666\n",
      "Test set accuracy : 0.9542999863624573\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(math.ceil(len(X_train) / batch_size)):\n",
    "            X_batch, y_batch = get_next_batch(iteration * batch_size, batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_val, y: y_val})\n",
    "        print('epoch: {}, training accuracy: {}, validation accuracy: {}'.format(epoch, acc_train, acc_val))\n",
    "    print('Test set accuracy : {}'.format(accuracy.eval(feed_dict={X: test_batch_data, y: test_batch_label})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
