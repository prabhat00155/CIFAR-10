{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\prroy\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 32\n",
    "width = 32\n",
    "channels = 3\n",
    "\n",
    "n_inputs = height * width * channels\n",
    "\n",
    "conv1_filters = 96\n",
    "conv1_ksize = 3\n",
    "conv1_stride = 1\n",
    "conv1_padding = 'SAME'\n",
    "\n",
    "conv2_filters = 96\n",
    "conv2_ksize = 3\n",
    "conv2_stride = 2\n",
    "conv2_padding = 'SAME'\n",
    "\n",
    "conv3_filters = 192\n",
    "conv3_ksize = 3\n",
    "conv3_stride = 1\n",
    "conv3_padding = 'SAME'\n",
    "\n",
    "conv4_filters = 192\n",
    "conv4_ksize = 3\n",
    "conv4_stride = 2\n",
    "conv4_padding = 'SAME'\n",
    "\n",
    "conv5_filters = 192\n",
    "conv5_ksize = 1\n",
    "conv5_stride = 1\n",
    "conv5_padding = 'VALID'\n",
    "\n",
    "conv6_filters = 10\n",
    "conv6_ksize = 1\n",
    "conv6_stride = 1\n",
    "conv6_padding = 'VALID'\n",
    "\n",
    "n_outputs = 10\n",
    "\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(name='X', dtype=tf.float32, shape=(None, height, width, channels))\n",
    "y = tf.placeholder(name='y', dtype=tf.int32, shape=(None))\n",
    "\n",
    "conv1 = tf.layers.conv2d(X, filters=conv1_filters, kernel_size=conv1_ksize, strides=conv1_stride, padding=conv1_padding,\n",
    "                        name='conv1', activation=tf.nn.relu)\n",
    "conv2 = tf.layers.conv2d(conv1, filters=conv1_filters, kernel_size=conv1_ksize, strides=conv1_stride, padding=conv1_padding,\n",
    "                        name='conv2', activation=tf.nn.relu)\n",
    "conv3 = tf.layers.conv2d(conv2, filters=conv2_filters, kernel_size=conv2_ksize, strides=conv2_stride, padding=conv2_padding,\n",
    "                        name='conv3', activation=tf.nn.relu)\n",
    "drop1 = tf.layers.dropout(conv3, rate=0.5)\n",
    "conv4 = tf.layers.conv2d(drop1, filters=conv3_filters, kernel_size=conv3_ksize, strides=conv3_stride, padding=conv3_padding,\n",
    "                       name='conv4', activation=tf.nn.relu)\n",
    "conv5 = tf.layers.conv2d(conv4, filters=conv3_filters, kernel_size=conv3_ksize, strides=conv3_stride, padding=conv3_padding,\n",
    "                        name='conv5', activation=tf.nn.relu)\n",
    "conv6 = tf.layers.conv2d(conv5, filters=conv4_filters, kernel_size=conv4_ksize, strides=conv4_stride, padding=conv4_padding,\n",
    "                       name='conv6', activation=tf.nn.relu)\n",
    "drop2 = tf.layers.dropout(conv6, rate=0.5)\n",
    "conv7 = tf.layers.conv2d(drop2, filters=conv3_filters, kernel_size=conv3_ksize, strides=conv3_stride, padding=conv3_padding,\n",
    "                        name='conv7', activation=tf.nn.relu)\n",
    "conv8 = tf.layers.conv2d(conv7, filters=conv5_filters, kernel_size=conv5_ksize, strides=conv5_stride, padding=conv5_padding,\n",
    "                        name='conv8', activation=tf.nn.relu)\n",
    "conv9 = tf.layers.conv2d(conv8, filters=conv6_filters, kernel_size=conv6_ksize, strides=conv6_stride, padding=conv6_padding,\n",
    "                        name='conv9')\n",
    "pool1 = tf.layers.average_pooling2d(conv9, pool_size=8, strides=8, padding='VALID')\n",
    "#print(pool1.shape)\n",
    "logits = tf.reshape(pool1, shape=(-1, 10))\n",
    "y_prob = tf.nn.softmax(logits, name='y_prob')\n",
    "\n",
    "learning_rate = 1e-4\n",
    "#momentum = 0.9\n",
    "\n",
    "\n",
    "with tf.name_scope('Train'):\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y)\n",
    "    loss = tf.reduce_mean(cross_entropy)\n",
    "    #optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=momentum )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        dict = pickle.load(fo, encoding='bytes')\n",
    "    return dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = []\n",
    "batches.append(unpickle('../../../../prroy/datasets/CIFAR10/cifar-10-batches-py/data_batch_1'))\n",
    "batches.append(unpickle('../../../../prroy/datasets/CIFAR10/cifar-10-batches-py/data_batch_2'))\n",
    "batches.append(unpickle('../../../../prroy/datasets/CIFAR10/cifar-10-batches-py/data_batch_3'))\n",
    "batches.append(unpickle('../../../../prroy/datasets/CIFAR10/cifar-10-batches-py/data_batch_4'))\n",
    "batches.append(unpickle('../../../../prroy/datasets/CIFAR10/cifar-10-batches-py/data_batch_5'))\n",
    "batches.append(unpickle('../../../../prroy/datasets/CIFAR10/cifar-10-batches-py/test_batch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_batch(batch):\n",
    "    return batches[batch][b'data'].reshape((-1,3,32,32)).transpose((0,2,3,1)), np.array(batches[batch][b'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch_data, test_batch_label = fetch_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch1, y_batch1 = fetch_batch(1)\n",
    "X_batch2, y_batch2 = fetch_batch(2)\n",
    "X_batch3, y_batch3 = fetch_batch(3)\n",
    "X_batch4, y_batch4 = fetch_batch(4)\n",
    "X_batch5, y_batch5 = fetch_batch(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 32, 32, 3) (10000, 32, 32, 3)\n"
     ]
    }
   ],
   "source": [
    "print(X_batch1.shape, X_batch2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data = np.concatenate((X_batch1, X_batch2, X_batch3, X_batch4, X_batch5))\n",
    "X_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(y_batch1.shape, y_batch2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_labels = np.concatenate((y_batch1, y_batch2, y_batch3, y_batch4, y_batch5))\n",
    "y_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_data, y_labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 32, 32, 3) (40000,) (10000, 32, 32, 3) (10000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_batch(starting_index, batch_size):\n",
    "    return X_train[starting_index: min(starting_index + batch_size, len(X_train))],\\\n",
    "        y_train[starting_index: min(starting_index + batch_size, len(y_train))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, training accuracy: 0.421875, validation accuracy: 0.44679999351501465\n",
      "Test set accuracy : 0.45010000467300415\n",
      "epoch: 1, training accuracy: 0.546875, validation accuracy: 0.5313000082969666\n",
      "Test set accuracy : 0.5408999919891357\n",
      "epoch: 2, training accuracy: 0.625, validation accuracy: 0.5871000289916992\n",
      "Test set accuracy : 0.6033999919891357\n",
      "epoch: 3, training accuracy: 0.640625, validation accuracy: 0.6236000061035156\n",
      "Test set accuracy : 0.6449999809265137\n",
      "epoch: 4, training accuracy: 0.703125, validation accuracy: 0.6444000005722046\n",
      "Test set accuracy : 0.6651999950408936\n",
      "epoch: 5, training accuracy: 0.78125, validation accuracy: 0.6704999804496765\n",
      "Test set accuracy : 0.6880999803543091\n",
      "epoch: 6, training accuracy: 0.8125, validation accuracy: 0.6787999868392944\n",
      "Test set accuracy : 0.703499972820282\n",
      "epoch: 7, training accuracy: 0.78125, validation accuracy: 0.6879000067710876\n",
      "Test set accuracy : 0.7128999829292297\n",
      "epoch: 8, training accuracy: 0.828125, validation accuracy: 0.6958000063896179\n",
      "Test set accuracy : 0.7233999967575073\n",
      "epoch: 9, training accuracy: 0.8125, validation accuracy: 0.7026000022888184\n",
      "Test set accuracy : 0.7322999835014343\n",
      "epoch: 10, training accuracy: 0.828125, validation accuracy: 0.7062000036239624\n",
      "Test set accuracy : 0.7400000095367432\n",
      "epoch: 11, training accuracy: 0.875, validation accuracy: 0.7098000049591064\n",
      "Test set accuracy : 0.7494000196456909\n",
      "epoch: 12, training accuracy: 0.875, validation accuracy: 0.710099995136261\n",
      "Test set accuracy : 0.7523000240325928\n",
      "epoch: 13, training accuracy: 0.875, validation accuracy: 0.7046999931335449\n",
      "Test set accuracy : 0.7516999840736389\n",
      "epoch: 14, training accuracy: 0.859375, validation accuracy: 0.7125999927520752\n",
      "Test set accuracy : 0.769599974155426\n",
      "epoch: 15, training accuracy: 0.953125, validation accuracy: 0.7121000289916992\n",
      "Test set accuracy : 0.7752000093460083\n",
      "epoch: 16, training accuracy: 0.953125, validation accuracy: 0.6988000273704529\n",
      "Test set accuracy : 0.76419997215271\n",
      "epoch: 17, training accuracy: 0.953125, validation accuracy: 0.7052000164985657\n",
      "Test set accuracy : 0.7750999927520752\n",
      "epoch: 18, training accuracy: 0.984375, validation accuracy: 0.7318999767303467\n",
      "Test set accuracy : 0.8043000102043152\n",
      "epoch: 19, training accuracy: 1.0, validation accuracy: 0.7376000285148621\n",
      "Test set accuracy : 0.8144999742507935\n",
      "epoch: 20, training accuracy: 1.0, validation accuracy: 0.7322999835014343\n",
      "Test set accuracy : 0.8166000247001648\n",
      "epoch: 21, training accuracy: 0.984375, validation accuracy: 0.7412999868392944\n",
      "Test set accuracy : 0.83160001039505\n",
      "epoch: 22, training accuracy: 1.0, validation accuracy: 0.7405999898910522\n",
      "Test set accuracy : 0.8300999999046326\n",
      "epoch: 23, training accuracy: 1.0, validation accuracy: 0.7283999919891357\n",
      "Test set accuracy : 0.8212000131607056\n",
      "epoch: 24, training accuracy: 0.984375, validation accuracy: 0.7240999937057495\n",
      "Test set accuracy : 0.8198999762535095\n",
      "epoch: 25, training accuracy: 1.0, validation accuracy: 0.7361000180244446\n",
      "Test set accuracy : 0.8363000154495239\n",
      "epoch: 26, training accuracy: 0.984375, validation accuracy: 0.7189000248908997\n",
      "Test set accuracy : 0.8156999945640564\n",
      "epoch: 27, training accuracy: 0.984375, validation accuracy: 0.7337999939918518\n",
      "Test set accuracy : 0.8517000079154968\n",
      "epoch: 28, training accuracy: 0.984375, validation accuracy: 0.7559000253677368\n",
      "Test set accuracy : 0.8826000094413757\n",
      "epoch: 29, training accuracy: 1.0, validation accuracy: 0.7324000000953674\n",
      "Test set accuracy : 0.8604999780654907\n",
      "epoch: 30, training accuracy: 0.984375, validation accuracy: 0.7425000071525574\n",
      "Test set accuracy : 0.8726999759674072\n",
      "epoch: 31, training accuracy: 1.0, validation accuracy: 0.7559000253677368\n",
      "Test set accuracy : 0.8939999938011169\n",
      "epoch: 32, training accuracy: 1.0, validation accuracy: 0.7621999979019165\n",
      "Test set accuracy : 0.899399995803833\n",
      "epoch: 33, training accuracy: 0.984375, validation accuracy: 0.753000020980835\n",
      "Test set accuracy : 0.8919000029563904\n",
      "epoch: 34, training accuracy: 1.0, validation accuracy: 0.7279000282287598\n",
      "Test set accuracy : 0.8633999824523926\n",
      "epoch: 35, training accuracy: 1.0, validation accuracy: 0.7330999970436096\n",
      "Test set accuracy : 0.8755000233650208\n",
      "epoch: 36, training accuracy: 1.0, validation accuracy: 0.7414000034332275\n",
      "Test set accuracy : 0.8934999704360962\n",
      "epoch: 37, training accuracy: 0.984375, validation accuracy: 0.7218999862670898\n",
      "Test set accuracy : 0.8553000092506409\n",
      "epoch: 38, training accuracy: 1.0, validation accuracy: 0.7333999872207642\n",
      "Test set accuracy : 0.8860999941825867\n",
      "epoch: 39, training accuracy: 1.0, validation accuracy: 0.7407000064849854\n",
      "Test set accuracy : 0.8852999806404114\n",
      "epoch: 40, training accuracy: 1.0, validation accuracy: 0.7217000126838684\n",
      "Test set accuracy : 0.8611000180244446\n",
      "epoch: 41, training accuracy: 0.96875, validation accuracy: 0.6873000264167786\n",
      "Test set accuracy : 0.8048999905586243\n",
      "epoch: 42, training accuracy: 1.0, validation accuracy: 0.7434999942779541\n",
      "Test set accuracy : 0.9135000109672546\n",
      "epoch: 43, training accuracy: 1.0, validation accuracy: 0.7581999897956848\n",
      "Test set accuracy : 0.9236000180244446\n",
      "epoch: 44, training accuracy: 1.0, validation accuracy: 0.756600022315979\n",
      "Test set accuracy : 0.9222999811172485\n",
      "epoch: 45, training accuracy: 1.0, validation accuracy: 0.7674999833106995\n",
      "Test set accuracy : 0.9358000159263611\n",
      "epoch: 46, training accuracy: 1.0, validation accuracy: 0.781000018119812\n",
      "Test set accuracy : 0.9478999972343445\n",
      "epoch: 47, training accuracy: 1.0, validation accuracy: 0.7821000218391418\n",
      "Test set accuracy : 0.949999988079071\n",
      "epoch: 48, training accuracy: 1.0, validation accuracy: 0.7835999727249146\n",
      "Test set accuracy : 0.9484999775886536\n",
      "epoch: 49, training accuracy: 1.0, validation accuracy: 0.7849000096321106\n",
      "Test set accuracy : 0.9503999948501587\n",
      "epoch: 50, training accuracy: 1.0, validation accuracy: 0.7803999781608582\n",
      "Test set accuracy : 0.9472000002861023\n",
      "epoch: 51, training accuracy: 1.0, validation accuracy: 0.7633000016212463\n",
      "Test set accuracy : 0.9373999834060669\n",
      "epoch: 52, training accuracy: 1.0, validation accuracy: 0.7627000212669373\n",
      "Test set accuracy : 0.9337999820709229\n",
      "epoch: 53, training accuracy: 1.0, validation accuracy: 0.7526000142097473\n",
      "Test set accuracy : 0.9254999756813049\n",
      "epoch: 54, training accuracy: 1.0, validation accuracy: 0.771399974822998\n",
      "Test set accuracy : 0.942799985408783\n",
      "epoch: 55, training accuracy: 1.0, validation accuracy: 0.7609000205993652\n",
      "Test set accuracy : 0.9318000078201294\n",
      "epoch: 56, training accuracy: 1.0, validation accuracy: 0.7534999847412109\n",
      "Test set accuracy : 0.9254999756813049\n",
      "epoch: 57, training accuracy: 1.0, validation accuracy: 0.7407000064849854\n",
      "Test set accuracy : 0.9178000092506409\n",
      "epoch: 58, training accuracy: 1.0, validation accuracy: 0.7652999758720398\n",
      "Test set accuracy : 0.941100001335144\n",
      "epoch: 59, training accuracy: 1.0, validation accuracy: 0.7656000256538391\n",
      "Test set accuracy : 0.9387999773025513\n",
      "epoch: 60, training accuracy: 1.0, validation accuracy: 0.762499988079071\n",
      "Test set accuracy : 0.9384999871253967\n",
      "epoch: 61, training accuracy: 1.0, validation accuracy: 0.7409999966621399\n",
      "Test set accuracy : 0.9133999943733215\n",
      "epoch: 62, training accuracy: 1.0, validation accuracy: 0.7698000073432922\n",
      "Test set accuracy : 0.9447000026702881\n",
      "epoch: 63, training accuracy: 1.0, validation accuracy: 0.7396000027656555\n",
      "Test set accuracy : 0.919700026512146\n",
      "epoch: 64, training accuracy: 1.0, validation accuracy: 0.7595999836921692\n",
      "Test set accuracy : 0.9398999810218811\n",
      "epoch: 65, training accuracy: 1.0, validation accuracy: 0.7549999952316284\n",
      "Test set accuracy : 0.9318000078201294\n",
      "epoch: 66, training accuracy: 1.0, validation accuracy: 0.7549999952316284\n",
      "Test set accuracy : 0.921500027179718\n",
      "epoch: 67, training accuracy: 1.0, validation accuracy: 0.7285000085830688\n",
      "Test set accuracy : 0.9049999713897705\n",
      "epoch: 68, training accuracy: 1.0, validation accuracy: 0.739799976348877\n",
      "Test set accuracy : 0.9121000170707703\n",
      "epoch: 69, training accuracy: 1.0, validation accuracy: 0.767799973487854\n",
      "Test set accuracy : 0.947700023651123\n",
      "epoch: 70, training accuracy: 1.0, validation accuracy: 0.7584999799728394\n",
      "Test set accuracy : 0.9438999891281128\n",
      "epoch: 71, training accuracy: 1.0, validation accuracy: 0.7717000246047974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracy : 0.9478999972343445\n",
      "epoch: 72, training accuracy: 1.0, validation accuracy: 0.7512999773025513\n",
      "Test set accuracy : 0.932200014591217\n",
      "epoch: 73, training accuracy: 1.0, validation accuracy: 0.772599995136261\n",
      "Test set accuracy : 0.9492999911308289\n",
      "epoch: 74, training accuracy: 1.0, validation accuracy: 0.7440999746322632\n",
      "Test set accuracy : 0.9208999872207642\n",
      "epoch: 75, training accuracy: 1.0, validation accuracy: 0.7756999731063843\n",
      "Test set accuracy : 0.9502999782562256\n",
      "epoch: 76, training accuracy: 1.0, validation accuracy: 0.7670000195503235\n",
      "Test set accuracy : 0.9429000020027161\n",
      "epoch: 77, training accuracy: 1.0, validation accuracy: 0.7469000220298767\n",
      "Test set accuracy : 0.9298999905586243\n",
      "epoch: 78, training accuracy: 1.0, validation accuracy: 0.762499988079071\n",
      "Test set accuracy : 0.9441999793052673\n",
      "epoch: 79, training accuracy: 1.0, validation accuracy: 0.7717000246047974\n",
      "Test set accuracy : 0.949400007724762\n",
      "epoch: 80, training accuracy: 1.0, validation accuracy: 0.781000018119812\n",
      "Test set accuracy : 0.9546999931335449\n",
      "epoch: 81, training accuracy: 1.0, validation accuracy: 0.758400022983551\n",
      "Test set accuracy : 0.9413999915122986\n",
      "epoch: 82, training accuracy: 1.0, validation accuracy: 0.7613000273704529\n",
      "Test set accuracy : 0.9370999932289124\n",
      "epoch: 83, training accuracy: 1.0, validation accuracy: 0.7731999754905701\n",
      "Test set accuracy : 0.9495000243186951\n",
      "epoch: 84, training accuracy: 1.0, validation accuracy: 0.767799973487854\n",
      "Test set accuracy : 0.9456999897956848\n",
      "epoch: 85, training accuracy: 1.0, validation accuracy: 0.7615000009536743\n",
      "Test set accuracy : 0.9398999810218811\n",
      "epoch: 86, training accuracy: 1.0, validation accuracy: 0.7666000127792358\n",
      "Test set accuracy : 0.9422000050544739\n",
      "epoch: 87, training accuracy: 1.0, validation accuracy: 0.7746000289916992\n",
      "Test set accuracy : 0.9517999887466431\n",
      "epoch: 88, training accuracy: 1.0, validation accuracy: 0.784500002861023\n",
      "Test set accuracy : 0.9567000269889832\n",
      "epoch: 89, training accuracy: 1.0, validation accuracy: 0.7645000219345093\n",
      "Test set accuracy : 0.9466000199317932\n",
      "epoch: 90, training accuracy: 1.0, validation accuracy: 0.7789000272750854\n",
      "Test set accuracy : 0.9549999833106995\n",
      "epoch: 91, training accuracy: 1.0, validation accuracy: 0.7648000121116638\n",
      "Test set accuracy : 0.9437999725341797\n",
      "epoch: 92, training accuracy: 1.0, validation accuracy: 0.7774999737739563\n",
      "Test set accuracy : 0.9544000029563904\n",
      "epoch: 93, training accuracy: 1.0, validation accuracy: 0.7865999937057495\n",
      "Test set accuracy : 0.9560999870300293\n",
      "epoch: 94, training accuracy: 1.0, validation accuracy: 0.7735999822616577\n",
      "Test set accuracy : 0.9506999850273132\n",
      "epoch: 95, training accuracy: 1.0, validation accuracy: 0.7533000111579895\n",
      "Test set accuracy : 0.9355000257492065\n",
      "epoch: 96, training accuracy: 1.0, validation accuracy: 0.7892000079154968\n",
      "Test set accuracy : 0.958299994468689\n",
      "epoch: 97, training accuracy: 1.0, validation accuracy: 0.7627999782562256\n",
      "Test set accuracy : 0.935699999332428\n",
      "epoch: 98, training accuracy: 1.0, validation accuracy: 0.7846999764442444\n",
      "Test set accuracy : 0.9544000029563904\n",
      "epoch: 99, training accuracy: 1.0, validation accuracy: 0.7786999940872192\n",
      "Test set accuracy : 0.9473000168800354\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 128\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(math.ceil(len(X_train) / batch_size)):\n",
    "            X_batch, y_batch = get_next_batch(iteration * batch_size, batch_size)\n",
    "            sess.run(training_op, feed_dict = {X: X_batch, y: y_batch})\n",
    "        acc_train = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "        acc_val = accuracy.eval(feed_dict={X: X_val, y: y_val})\n",
    "        print('epoch: {}, training accuracy: {}, validation accuracy: {}'.format(epoch, acc_train, acc_val))\n",
    "        print('Test set accuracy : {}'.format(accuracy.eval(feed_dict={X: test_batch_data, y: test_batch_label})))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
